{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/\n",
    "\n",
    "https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/\n",
    "\n",
    "https://machinelearningmastery.com/difference-between-a-batch-and-an-epoch/\n",
    "    \n",
    "https://machinelearningmastery.com/evaluate-skill-deep-learning-models/\n",
    "\n",
    "https://machinelearningmastery.com/how-to-make-classification-and-regression-predictions-for-deep-learning-models-in-keras/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pasos para poner a prueba una Deep Neural Net\n",
    "Load Data.\n",
    "Define Keras Model.\n",
    "Compile Keras Model.\n",
    "Fit Keras Model.\n",
    "Evaluate Keras Model.\n",
    "Tie It All Together.\n",
    "Make Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este ejercicio, vamos a utilizar el conjunto de datos de inicio de diabetes de los Pima Indians. Este es un conjunto de datos de aprendizaje automático estándar del repositorio de Aprendizaje automático de UCI. Describe los datos de los registros médicos de los pacientes de los Pima Indians y si tuvieron un inicio de diabetes dentro de los cinco años.\n",
    "\n",
    "Como tal, es un problema de clasificación binaria (aparición de diabetes como 1 o no como 0). Todas las variables de entrada que describen a cada paciente son numéricas. Esto facilita su uso directo con redes neuronales que esperan valores numéricos de entrada y salida, e ideal para nuestra primera red neuronal en Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descargar dataset en:\n",
    "    \n",
    "    https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\n",
    "    \n",
    "   \n",
    "Información del dataset en:\n",
    "\n",
    "    https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos cargar el archivo como una matriz de números usando la función NumPy loadtxt ().\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# first neural network with keras tutorial\n",
    "from numpy import loadtxt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Hay ocho variables de entrada y una variable de salida (la última columna). Aprenderemos un modelo para asignar filas de variables de entrada (X) a una variable de salida (y), que a menudo resumimos como y = f (X).\n",
    "\n",
    "Las variables se pueden resumir de la siguiente manera:\n",
    "\n",
    "Variables de entrada (X):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of times pregnant\n",
    "\n",
    "Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n",
    "\n",
    "Diastolic blood pressure (mm Hg)\n",
    "\n",
    "Triceps skin fold thickness (mm)\n",
    "\n",
    "2-Hour serum insulin (mu U/ml)\n",
    "\n",
    "Body mass index (weight in kg/(height in m)^2)\n",
    "\n",
    "Diabetes pedigree function\n",
    "\n",
    "Age (years)\n",
    "\n",
    "\n",
    "Variables de salida (y):\n",
    "\n",
    "Variable de clase (0 o 1)\n",
    "\n",
    "\n",
    "Una vez que el archivo CSV se carga en la memoria, podemos dividir las columnas de datos en variables de entrada y salida.\n",
    "\n",
    "Los datos se almacenarán en una matriz 2D donde la primera dimensión es filas y la segunda dimensión son columnas, p. [filas, columnas].\n",
    "\n",
    "Podemos dividir la matriz en dos matrices seleccionando subconjuntos de columnas usando el operador de división estándar NumPy o \":\" Podemos seleccionar las primeras 8 columnas del índice 0 al índice 7 a través de la división 0: 8. Luego podemos seleccionar la columna de salida (la novena variable) a través del índice 8.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# load the dataset\n",
    "dataset = loadtxt('pima-indians-diabetes.csv', delimiter=',')\n",
    "# split into input (X) and output (y) variables\n",
    "X = dataset[:,0:8]  #todas las filas, de la 0 a la 7 columna\n",
    "y = dataset[:,8]    #todas las filas, la columna 8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora estamos listos para definir nuestro modelo de red neuronal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Keras Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los modelos en Keras se definen como una secuencia de capas.\n",
    "\n",
    "Creamos un modelo secuencial y agregamos capas de una en una hasta que estemos contentos con nuestra arquitectura de red.\n",
    "\n",
    "Lo primero que debe acertar es asegurarse de que la capa de entrada (input layer) tenga el número correcto de características de entrada. Esto se puede especificar al crear la primera capa con el argumento input_dim y establecerlo en 8 para las 8 variables de entrada.\n",
    "\n",
    "¿Cómo sabemos la cantidad de capas y sus tipos?\n",
    "\n",
    "Esta es una pregunta muy difícil. Hay heurísticas que podemos usar y, a menudo, la mejor estructura de red se encuentra a través de un proceso de experimentación de prueba y error. Generalmente, necesita una red lo suficientemente grande como para capturar la estructura del problema.\n",
    "\n",
    "En este ejemplo, utilizaremos una estructura de red totalmente conectada con tres capas (fully-connected network structure with three layers).\n",
    "\n",
    "Las capas completamente conectadas se definen usando la clase Dense. Podemos especificar el número de neuronas o nodos en la capa como primer argumento, y especificar la función de activación (activation function) usando el argumento de activación.\n",
    "\n",
    "Utilizaremos la función de activación de unidad lineal rectificada denominada ReLU en las dos primeras capas y la función Sigmoide en la capa de salida.\n",
    "\n",
    "Solía ser el caso que las funciones de activación Sigmoid y Tanh eran preferidas para todas las capas. En estos días, se logra un mejor rendimiento utilizando la función de activación ReLU. Utilizamos un sigmoide en la capa de salida para garantizar que nuestra salida de red esté entre 0 y 1 y sea fácil de asignar a una probabilidad de clase 1 o ajustar a una clasificación rígida de cualquiera de las clases con un umbral predeterminado de 0.5.\n",
    "\n",
    "Podemos juntar todo agregando cada capa:\n",
    "\n",
    "El modelo espera filas de datos con 8 variables (el argumento input_dim = 8)\n",
    "La primera capa oculta tiene 12 nodos y usa la función de activación relu.\n",
    "La segunda capa oculta tiene 8 nodos y usa la función de activación relu.\n",
    "La capa de salida tiene un nodo y usa la función de activación sigmoidea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# define the keras model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=8, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 12)                108       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 104       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 221\n",
      "Trainable params: 221\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenga en cuenta que lo más confuso aquí es que la forma de la entrada al modelo se define como un argumento en la primera capa oculta. Esto significa que la línea de código que agrega la primera capa densa está haciendo 2 cosas, definiendo la capa de entrada o visible y la primera capa oculta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El resumen es textual e incluye información sobre:\n",
    "\n",
    "- Las capas y su orden en el modelo.\n",
    "- La forma (shape) de salida de cada capa.\n",
    "- El número de parámetros (pesos) en cada capa.\n",
    "- El número total de parámetros (pesos) en el modelo.\n",
    "\n",
    "El resumen se puede crear llamando a la función summary () en el modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile Keras Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que el modelo está definido, podemos compilarlo.\n",
    "\n",
    "La compilación del modelo utiliza las bibliotecas numéricas eficientes (el llamado backend) como Theano o TensorFlow. El backend elige automáticamente la mejor manera de representar la red para el entrenamiento y hacer predicciones para ejecutar en su hardware, como CPU o GPU o incluso distribuidas.\n",
    "\n",
    "Al compilar, debemos especificar algunas propiedades adicionales requeridas al entrenar la red. Recuerde que entrenar una red significa encontrar el mejor conjunto de pesos para mapear entradas a salidas en nuestro conjunto de datos.\n",
    "\n",
    "Debemos especificar la función de pérdida (loss function) que se utilizará para evaluar un conjunto de pesos, el optimizador se usa para buscar a través de diferentes pesos para la red y cualquier métrica opcional que nos gustaría recopilar e informar durante el entrenamiento.\n",
    "\n",
    "En este caso, utilizaremos la entropía cruzada (cross entropy) como argumento de pérdida. Esta pérdida es para problemas de clasificación binaria y se define en Keras como \"binary_crossentropy\". \n",
    "\n",
    "Definiremos el optimizador como el algoritmo eficiente de descenso de gradiente estocástico \"adam\". Esta es una versión popular del descenso de gradiente porque se sintoniza automáticamente y brinda buenos resultados en una amplia gama de problemas. \n",
    "\n",
    "Finalmente, debido a que es un problema de clasificación, recopilaremos e informaremos la 'accuracy' de la clasificación, definida a través del argumento metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# compile the keras model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit Keras Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hemos definido nuestro modelo y lo hemos compilado para un cálculo eficiente.\n",
    "\n",
    "Ahora es el momento de ejecutar el modelo en algunos datos.\n",
    "\n",
    "Podemos entrenar o ajustar nuestro modelo en nuestros datos cargados llamando a la función fit () en el modelo.\n",
    "\n",
    "El entrenamiento ocurre durante épocas y cada época se divide en lotes.\n",
    "\n",
    "Epochs: una pasada a través de todas las filas del conjunto de datos de entrenamiento.\n",
    "Batch: una o más muestras consideradas por el modelo dentro de una época antes de actualizar los pesos.\n",
    "\n",
    "\n",
    "Una época se compone de uno o más lotes, según el tamaño de lote elegido y el modelo es apto para muchas épocas. \n",
    "El proceso de entrenamiento se ejecutará durante un número fijo de iteraciones a través del conjunto de datos llamado epochs, que debemos especificar usando el argumento epochs. También debemos establecer el número de filas del conjunto de datos que se consideran antes de que los pesos del modelo se actualicen dentro de cada época, denominado tamaño del lote y establecido mediante el argumento batch_size.\n",
    "\n",
    "Para este problema, ejecutaremos una pequeña cantidad de épocas (150) y usaremos un tamaño de lote relativamente pequeño de 10. Esto significa que cada época implicará (150/10) 15 actualizaciones de los pesos del modelo.\n",
    "\n",
    "Estas configuraciones se pueden elegir experimentalmente por prueba y error. Queremos entrenar al modelo lo suficiente para que aprenda una buena (o suficiente) asignación de filas de datos de entrada a la clasificación de salida. El modelo siempre tendrá algún error, pero la cantidad de error se nivelará después de algún punto para una configuración de modelo dada. Esto se llama convergencia modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "768/768 [==============================] - 0s 486us/step - loss: 3.9251 - accuracy: 0.6302\n",
      "Epoch 2/150\n",
      "768/768 [==============================] - 0s 163us/step - loss: 2.5586 - accuracy: 0.6367\n",
      "Epoch 3/150\n",
      "768/768 [==============================] - 0s 173us/step - loss: 1.7033 - accuracy: 0.6289\n",
      "Epoch 4/150\n",
      "768/768 [==============================] - 0s 161us/step - loss: 1.1804 - accuracy: 0.6224\n",
      "Epoch 5/150\n",
      "768/768 [==============================] - 0s 159us/step - loss: 0.9575 - accuracy: 0.6146\n",
      "Epoch 6/150\n",
      "768/768 [==============================] - 0s 167us/step - loss: 0.8499 - accuracy: 0.6263\n",
      "Epoch 7/150\n",
      "768/768 [==============================] - 0s 179us/step - loss: 0.7941 - accuracy: 0.6393\n",
      "Epoch 8/150\n",
      "768/768 [==============================] - 0s 179us/step - loss: 0.7501 - accuracy: 0.6419\n",
      "Epoch 9/150\n",
      "768/768 [==============================] - 0s 185us/step - loss: 0.7104 - accuracy: 0.6549\n",
      "Epoch 10/150\n",
      "768/768 [==============================] - 0s 176us/step - loss: 0.6912 - accuracy: 0.6641\n",
      "Epoch 11/150\n",
      "768/768 [==============================] - 0s 167us/step - loss: 0.6711 - accuracy: 0.6693\n",
      "Epoch 12/150\n",
      "768/768 [==============================] - 0s 164us/step - loss: 0.6532 - accuracy: 0.6927\n",
      "Epoch 13/150\n",
      "768/768 [==============================] - 0s 198us/step - loss: 0.6320 - accuracy: 0.6758\n",
      "Epoch 14/150\n",
      "768/768 [==============================] - 0s 231us/step - loss: 0.6295 - accuracy: 0.6836\n",
      "Epoch 15/150\n",
      "768/768 [==============================] - 0s 224us/step - loss: 0.6107 - accuracy: 0.6966\n",
      "Epoch 16/150\n",
      "768/768 [==============================] - 0s 180us/step - loss: 0.6172 - accuracy: 0.6927\n",
      "Epoch 17/150\n",
      "768/768 [==============================] - 0s 167us/step - loss: 0.6046 - accuracy: 0.7057\n",
      "Epoch 18/150\n",
      "768/768 [==============================] - 0s 167us/step - loss: 0.6010 - accuracy: 0.6927\n",
      "Epoch 19/150\n",
      "768/768 [==============================] - 0s 170us/step - loss: 0.6007 - accuracy: 0.6927\n",
      "Epoch 20/150\n",
      "768/768 [==============================] - 0s 183us/step - loss: 0.5991 - accuracy: 0.6953\n",
      "Epoch 21/150\n",
      "768/768 [==============================] - 0s 185us/step - loss: 0.5887 - accuracy: 0.7070\n",
      "Epoch 22/150\n",
      "768/768 [==============================] - 0s 188us/step - loss: 0.5933 - accuracy: 0.6966\n",
      "Epoch 23/150\n",
      "768/768 [==============================] - 0s 192us/step - loss: 0.5882 - accuracy: 0.7005\n",
      "Epoch 24/150\n",
      "768/768 [==============================] - 0s 186us/step - loss: 0.6031 - accuracy: 0.6914\n",
      "Epoch 25/150\n",
      "768/768 [==============================] - 0s 189us/step - loss: 0.5823 - accuracy: 0.7070\n",
      "Epoch 26/150\n",
      "768/768 [==============================] - 0s 196us/step - loss: 0.5916 - accuracy: 0.6966\n",
      "Epoch 27/150\n",
      "768/768 [==============================] - 0s 192us/step - loss: 0.5790 - accuracy: 0.7083\n",
      "Epoch 28/150\n",
      "768/768 [==============================] - 0s 157us/step - loss: 0.5802 - accuracy: 0.7161\n",
      "Epoch 29/150\n",
      "768/768 [==============================] - 0s 162us/step - loss: 0.5876 - accuracy: 0.7161\n",
      "Epoch 30/150\n",
      "768/768 [==============================] - 0s 177us/step - loss: 0.5777 - accuracy: 0.7057\n",
      "Epoch 31/150\n",
      "768/768 [==============================] - 0s 156us/step - loss: 0.5736 - accuracy: 0.7148\n",
      "Epoch 32/150\n",
      "768/768 [==============================] - 0s 169us/step - loss: 0.5724 - accuracy: 0.7201\n",
      "Epoch 33/150\n",
      "768/768 [==============================] - 0s 186us/step - loss: 0.5713 - accuracy: 0.7148\n",
      "Epoch 34/150\n",
      "768/768 [==============================] - 0s 183us/step - loss: 0.5649 - accuracy: 0.7057\n",
      "Epoch 35/150\n",
      "768/768 [==============================] - 0s 174us/step - loss: 0.5787 - accuracy: 0.7070\n",
      "Epoch 36/150\n",
      "768/768 [==============================] - 0s 168us/step - loss: 0.5551 - accuracy: 0.7383\n",
      "Epoch 37/150\n",
      "768/768 [==============================] - 0s 191us/step - loss: 0.5622 - accuracy: 0.7214\n",
      "Epoch 38/150\n",
      "768/768 [==============================] - 0s 171us/step - loss: 0.5706 - accuracy: 0.7214\n",
      "Epoch 39/150\n",
      "768/768 [==============================] - 0s 165us/step - loss: 0.5664 - accuracy: 0.7188\n",
      "Epoch 40/150\n",
      "768/768 [==============================] - 0s 172us/step - loss: 0.5679 - accuracy: 0.7031\n",
      "Epoch 41/150\n",
      "768/768 [==============================] - 0s 184us/step - loss: 0.5603 - accuracy: 0.7188\n",
      "Epoch 42/150\n",
      "768/768 [==============================] - 0s 286us/step - loss: 0.5507 - accuracy: 0.7292\n",
      "Epoch 43/150\n",
      "768/768 [==============================] - 0s 209us/step - loss: 0.5574 - accuracy: 0.7227\n",
      "Epoch 44/150\n",
      "768/768 [==============================] - 0s 172us/step - loss: 0.5571 - accuracy: 0.7148\n",
      "Epoch 45/150\n",
      "768/768 [==============================] - 0s 210us/step - loss: 0.5616 - accuracy: 0.7292\n",
      "Epoch 46/150\n",
      "768/768 [==============================] - 0s 269us/step - loss: 0.5571 - accuracy: 0.7292\n",
      "Epoch 47/150\n",
      "768/768 [==============================] - 0s 225us/step - loss: 0.5561 - accuracy: 0.7357\n",
      "Epoch 48/150\n",
      "768/768 [==============================] - 0s 275us/step - loss: 0.5462 - accuracy: 0.7292\n",
      "Epoch 49/150\n",
      "768/768 [==============================] - 0s 385us/step - loss: 0.5404 - accuracy: 0.7357\n",
      "Epoch 50/150\n",
      "768/768 [==============================] - 0s 330us/step - loss: 0.5532 - accuracy: 0.7331\n",
      "Epoch 51/150\n",
      "768/768 [==============================] - 0s 320us/step - loss: 0.5542 - accuracy: 0.7396\n",
      "Epoch 52/150\n",
      "768/768 [==============================] - 0s 208us/step - loss: 0.5491 - accuracy: 0.7305\n",
      "Epoch 53/150\n",
      "768/768 [==============================] - 0s 214us/step - loss: 0.5498 - accuracy: 0.7331\n",
      "Epoch 54/150\n",
      "768/768 [==============================] - 0s 168us/step - loss: 0.5390 - accuracy: 0.7422\n",
      "Epoch 55/150\n",
      "768/768 [==============================] - 0s 142us/step - loss: 0.5397 - accuracy: 0.7344\n",
      "Epoch 56/150\n",
      "768/768 [==============================] - 0s 140us/step - loss: 0.5412 - accuracy: 0.7383\n",
      "Epoch 57/150\n",
      "768/768 [==============================] - 0s 154us/step - loss: 0.5441 - accuracy: 0.7448\n",
      "Epoch 58/150\n",
      "768/768 [==============================] - 0s 299us/step - loss: 0.5370 - accuracy: 0.7331\n",
      "Epoch 59/150\n",
      "768/768 [==============================] - 0s 235us/step - loss: 0.5419 - accuracy: 0.7214\n",
      "Epoch 60/150\n",
      "768/768 [==============================] - 0s 192us/step - loss: 0.5377 - accuracy: 0.7305\n",
      "Epoch 61/150\n",
      "768/768 [==============================] - 0s 142us/step - loss: 0.5433 - accuracy: 0.7370\n",
      "Epoch 62/150\n",
      "768/768 [==============================] - 0s 142us/step - loss: 0.5342 - accuracy: 0.7539\n",
      "Epoch 63/150\n",
      "768/768 [==============================] - 0s 140us/step - loss: 0.5329 - accuracy: 0.7539\n",
      "Epoch 64/150\n",
      "768/768 [==============================] - 0s 139us/step - loss: 0.5396 - accuracy: 0.7422\n",
      "Epoch 65/150\n",
      "768/768 [==============================] - 0s 149us/step - loss: 0.5356 - accuracy: 0.7474\n",
      "Epoch 66/150\n",
      "768/768 [==============================] - 0s 146us/step - loss: 0.5377 - accuracy: 0.7240\n",
      "Epoch 67/150\n",
      "768/768 [==============================] - 0s 155us/step - loss: 0.5270 - accuracy: 0.7357\n",
      "Epoch 68/150\n",
      "768/768 [==============================] - 0s 161us/step - loss: 0.5341 - accuracy: 0.7435\n",
      "Epoch 69/150\n",
      "768/768 [==============================] - 0s 147us/step - loss: 0.5389 - accuracy: 0.7448\n",
      "Epoch 70/150\n",
      "768/768 [==============================] - 0s 137us/step - loss: 0.5315 - accuracy: 0.7383\n",
      "Epoch 71/150\n",
      "768/768 [==============================] - 0s 143us/step - loss: 0.5365 - accuracy: 0.7409\n",
      "Epoch 72/150\n",
      "768/768 [==============================] - 0s 165us/step - loss: 0.5256 - accuracy: 0.7370\n",
      "Epoch 73/150\n",
      "768/768 [==============================] - 0s 182us/step - loss: 0.5269 - accuracy: 0.7565\n",
      "Epoch 74/150\n",
      "768/768 [==============================] - 0s 155us/step - loss: 0.5288 - accuracy: 0.7487\n",
      "Epoch 75/150\n",
      "768/768 [==============================] - 0s 155us/step - loss: 0.5352 - accuracy: 0.7448\n",
      "Epoch 76/150\n",
      "768/768 [==============================] - 0s 169us/step - loss: 0.5361 - accuracy: 0.7370\n",
      "Epoch 77/150\n",
      "768/768 [==============================] - 0s 160us/step - loss: 0.5297 - accuracy: 0.7461\n",
      "Epoch 78/150\n",
      "768/768 [==============================] - 0s 141us/step - loss: 0.5306 - accuracy: 0.7409\n",
      "Epoch 79/150\n",
      "768/768 [==============================] - 0s 181us/step - loss: 0.5249 - accuracy: 0.7396\n",
      "Epoch 80/150\n",
      "768/768 [==============================] - 0s 186us/step - loss: 0.5197 - accuracy: 0.7448\n",
      "Epoch 81/150\n",
      "768/768 [==============================] - 0s 461us/step - loss: 0.5202 - accuracy: 0.7474\n",
      "Epoch 82/150\n",
      "768/768 [==============================] - 0s 279us/step - loss: 0.5340 - accuracy: 0.7409\n",
      "Epoch 83/150\n",
      "768/768 [==============================] - 0s 160us/step - loss: 0.5274 - accuracy: 0.7409\n",
      "Epoch 84/150\n",
      "768/768 [==============================] - 0s 148us/step - loss: 0.5169 - accuracy: 0.7539\n",
      "Epoch 85/150\n",
      "768/768 [==============================] - 0s 168us/step - loss: 0.5291 - accuracy: 0.7513\n",
      "Epoch 86/150\n",
      "768/768 [==============================] - 0s 158us/step - loss: 0.5216 - accuracy: 0.7539\n",
      "Epoch 87/150\n",
      "768/768 [==============================] - 0s 168us/step - loss: 0.5285 - accuracy: 0.7409\n",
      "Epoch 88/150\n",
      "768/768 [==============================] - 0s 161us/step - loss: 0.5187 - accuracy: 0.7461\n",
      "Epoch 89/150\n",
      "768/768 [==============================] - 0s 159us/step - loss: 0.5172 - accuracy: 0.7422\n",
      "Epoch 90/150\n",
      "768/768 [==============================] - 0s 155us/step - loss: 0.5251 - accuracy: 0.7370\n",
      "Epoch 91/150\n",
      "768/768 [==============================] - 0s 154us/step - loss: 0.5214 - accuracy: 0.7539\n",
      "Epoch 92/150\n",
      "768/768 [==============================] - 0s 150us/step - loss: 0.5134 - accuracy: 0.7474\n",
      "Epoch 93/150\n",
      "768/768 [==============================] - 0s 139us/step - loss: 0.5186 - accuracy: 0.7461\n",
      "Epoch 94/150\n",
      "768/768 [==============================] - 0s 139us/step - loss: 0.5175 - accuracy: 0.7461\n",
      "Epoch 95/150\n",
      "768/768 [==============================] - 0s 152us/step - loss: 0.5178 - accuracy: 0.7435\n",
      "Epoch 96/150\n",
      "768/768 [==============================] - 0s 157us/step - loss: 0.5249 - accuracy: 0.7461\n",
      "Epoch 97/150\n",
      "768/768 [==============================] - 0s 160us/step - loss: 0.5148 - accuracy: 0.7331\n",
      "Epoch 98/150\n",
      "768/768 [==============================] - 0s 144us/step - loss: 0.5101 - accuracy: 0.7526\n",
      "Epoch 99/150\n",
      "768/768 [==============================] - 0s 137us/step - loss: 0.5150 - accuracy: 0.7409\n",
      "Epoch 100/150\n",
      "768/768 [==============================] - 0s 140us/step - loss: 0.5106 - accuracy: 0.7565\n",
      "Epoch 101/150\n",
      "768/768 [==============================] - 0s 142us/step - loss: 0.5095 - accuracy: 0.7539\n",
      "Epoch 102/150\n",
      "768/768 [==============================] - 0s 149us/step - loss: 0.5133 - accuracy: 0.7435\n",
      "Epoch 103/150\n",
      "768/768 [==============================] - 0s 155us/step - loss: 0.5356 - accuracy: 0.7253\n",
      "Epoch 104/150\n",
      "768/768 [==============================] - 0s 152us/step - loss: 0.5035 - accuracy: 0.7487\n",
      "Epoch 105/150\n",
      "768/768 [==============================] - 0s 137us/step - loss: 0.5075 - accuracy: 0.7487\n",
      "Epoch 106/150\n",
      "768/768 [==============================] - 0s 139us/step - loss: 0.5073 - accuracy: 0.7487\n",
      "Epoch 107/150\n",
      "768/768 [==============================] - 0s 140us/step - loss: 0.5184 - accuracy: 0.7435\n",
      "Epoch 108/150\n",
      "768/768 [==============================] - 0s 153us/step - loss: 0.5050 - accuracy: 0.7487\n",
      "Epoch 109/150\n",
      "768/768 [==============================] - 0s 152us/step - loss: 0.5126 - accuracy: 0.7539\n",
      "Epoch 110/150\n",
      "768/768 [==============================] - 0s 154us/step - loss: 0.5178 - accuracy: 0.7448\n",
      "Epoch 111/150\n",
      "768/768 [==============================] - 0s 153us/step - loss: 0.5084 - accuracy: 0.7448\n",
      "Epoch 112/150\n",
      "768/768 [==============================] - 0s 157us/step - loss: 0.5051 - accuracy: 0.7500\n",
      "Epoch 113/150\n",
      "768/768 [==============================] - 0s 156us/step - loss: 0.5110 - accuracy: 0.7500\n",
      "Epoch 114/150\n",
      "768/768 [==============================] - 0s 158us/step - loss: 0.5039 - accuracy: 0.7539\n",
      "Epoch 115/150\n",
      "768/768 [==============================] - 0s 150us/step - loss: 0.5088 - accuracy: 0.7526\n",
      "Epoch 116/150\n",
      "768/768 [==============================] - 0s 160us/step - loss: 0.5189 - accuracy: 0.7435\n",
      "Epoch 117/150\n",
      "768/768 [==============================] - 0s 159us/step - loss: 0.5102 - accuracy: 0.7422\n",
      "Epoch 118/150\n",
      "768/768 [==============================] - 0s 153us/step - loss: 0.5025 - accuracy: 0.7604\n",
      "Epoch 119/150\n",
      "768/768 [==============================] - 0s 134us/step - loss: 0.4996 - accuracy: 0.7617\n",
      "Epoch 120/150\n",
      "768/768 [==============================] - 0s 141us/step - loss: 0.5029 - accuracy: 0.7617\n",
      "Epoch 121/150\n",
      "768/768 [==============================] - 0s 148us/step - loss: 0.5025 - accuracy: 0.7708\n",
      "Epoch 122/150\n",
      "768/768 [==============================] - 0s 156us/step - loss: 0.4991 - accuracy: 0.7604\n",
      "Epoch 123/150\n",
      "768/768 [==============================] - 0s 154us/step - loss: 0.4970 - accuracy: 0.7669\n",
      "Epoch 124/150\n",
      "768/768 [==============================] - 0s 156us/step - loss: 0.5091 - accuracy: 0.7513\n",
      "Epoch 125/150\n",
      "768/768 [==============================] - 0s 153us/step - loss: 0.5068 - accuracy: 0.7552\n",
      "Epoch 126/150\n",
      "768/768 [==============================] - 0s 145us/step - loss: 0.5000 - accuracy: 0.7682\n",
      "Epoch 127/150\n",
      "768/768 [==============================] - 0s 139us/step - loss: 0.4961 - accuracy: 0.7604\n",
      "Epoch 128/150\n",
      "768/768 [==============================] - 0s 143us/step - loss: 0.5027 - accuracy: 0.7578\n",
      "Epoch 129/150\n",
      "768/768 [==============================] - 0s 154us/step - loss: 0.4988 - accuracy: 0.7539\n",
      "Epoch 130/150\n",
      "768/768 [==============================] - 0s 155us/step - loss: 0.4954 - accuracy: 0.7591\n",
      "Epoch 131/150\n",
      "768/768 [==============================] - 0s 140us/step - loss: 0.4999 - accuracy: 0.7565\n",
      "Epoch 132/150\n",
      "768/768 [==============================] - 0s 141us/step - loss: 0.5010 - accuracy: 0.7565\n",
      "Epoch 133/150\n",
      "768/768 [==============================] - 0s 147us/step - loss: 0.5030 - accuracy: 0.7617\n",
      "Epoch 134/150\n",
      "768/768 [==============================] - 0s 153us/step - loss: 0.5064 - accuracy: 0.7578\n",
      "Epoch 135/150\n",
      "768/768 [==============================] - 0s 154us/step - loss: 0.5085 - accuracy: 0.7500\n",
      "Epoch 136/150\n",
      "768/768 [==============================] - 0s 154us/step - loss: 0.5003 - accuracy: 0.7565\n",
      "Epoch 137/150\n",
      "768/768 [==============================] - 0s 151us/step - loss: 0.5028 - accuracy: 0.7669\n",
      "Epoch 138/150\n",
      "768/768 [==============================] - 0s 140us/step - loss: 0.4910 - accuracy: 0.7734\n",
      "Epoch 139/150\n",
      "768/768 [==============================] - 0s 138us/step - loss: 0.4957 - accuracy: 0.7591\n",
      "Epoch 140/150\n",
      "768/768 [==============================] - 0s 151us/step - loss: 0.4963 - accuracy: 0.7682\n",
      "Epoch 141/150\n",
      "768/768 [==============================] - 0s 155us/step - loss: 0.4992 - accuracy: 0.7578\n",
      "Epoch 142/150\n",
      "768/768 [==============================] - 0s 154us/step - loss: 0.4931 - accuracy: 0.7643\n",
      "Epoch 143/150\n",
      "768/768 [==============================] - 0s 157us/step - loss: 0.4895 - accuracy: 0.7565\n",
      "Epoch 144/150\n",
      "768/768 [==============================] - 0s 154us/step - loss: 0.5034 - accuracy: 0.7513\n",
      "Epoch 145/150\n",
      "768/768 [==============================] - 0s 137us/step - loss: 0.5000 - accuracy: 0.7682\n",
      "Epoch 146/150\n",
      "768/768 [==============================] - 0s 138us/step - loss: 0.4950 - accuracy: 0.7656\n",
      "Epoch 147/150\n",
      "768/768 [==============================] - 0s 148us/step - loss: 0.4897 - accuracy: 0.7643\n",
      "Epoch 148/150\n",
      "768/768 [==============================] - 0s 154us/step - loss: 0.4977 - accuracy: 0.7656\n",
      "Epoch 149/150\n",
      "768/768 [==============================] - 0s 155us/step - loss: 0.4992 - accuracy: 0.7448\n",
      "Epoch 150/150\n",
      "768/768 [==============================] - 0s 153us/step - loss: 0.4943 - accuracy: 0.7591\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x128e1ca90>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# fit the keras model on the dataset\n",
    "model.fit(X, y, epochs=150, batch_size=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Keras Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hemos entrenado nuestra red neuronal en todo el conjunto de datos y podemos evaluar el rendimiento de la red en el mismo conjunto de datos.\n",
    "\n",
    "Esto solo nos dará una idea de qué tan bien hemos modelado el conjunto de datos, pero no tenemos idea de qué tan bien podría funcionar el algoritmo con los nuevos datos. Lo hemos hecho por simplicidad, pero idealmente, podría separar sus datos en conjuntos de datos de entrenamiento y prueba.\n",
    "\n",
    "Puede evaluar su modelo en su conjunto de datos de entrenamiento utilizando la función evaluar() en su modelo y pasarle la misma entrada y salida utilizada para entrenar el modelo.\n",
    "\n",
    "Esto generará una predicción para cada par de entrada y salida y recopilará puntajes, incluida la pérdida promedio y cualquier métrica que haya configurado, como la precisión.\n",
    "\n",
    "La función evaluar() devolverá una lista con dos valores. La primera será la pérdida del modelo en el conjunto de datos y la segunda será la precisión del modelo en el conjunto de datos. Solo estamos interesados en informar la precisión, por lo que ignoraremos el valor de la pérdida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768/768 [==============================] - 0s 62us/step\n",
      "Accuracy: 75.13\n"
     ]
    }
   ],
   "source": [
    "# evaluate the keras model\n",
    "_, accuracy = model.evaluate(X, y)\n",
    "print('Accuracy: %.2f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos adaptar el ejemplo anterior y usarlo para generar predicciones en el conjunto de datos de entrenamiento, pretendiendo que es un nuevo conjunto de datos que no hemos visto antes.\n",
    "\n",
    "Hacer predicciones es tan fácil como llamar a la función predict() en el modelo. Estamos utilizando una función de activación sigmoidea en la capa de salida, por lo que las predicciones serán una probabilidad en el rango entre 0 y 1. Podemos convertirlas fácilmente en una predicción binaria nítida para esta tarea de clasificación redondeándolas.\n",
    "\n",
    "Por ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# make probability predictions with the model\n",
    "predictions = model.predict(X)\n",
    "# round predictions \n",
    "rounded = [round(x[0]) for x in predictions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternativamente, podemos llamar a la función predict_classes () en el modelo para predecir clases nítidas directamente, por ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# make class predictions with the model\n",
    "predictions = model.predict_classes(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.0, 148.0, 72.0, 35.0, 0.0, 33.6, 0.627, 50.0] => 1 (expected 1)\n",
      "[1.0, 85.0, 66.0, 29.0, 0.0, 26.6, 0.351, 31.0] => 0 (expected 0)\n",
      "[8.0, 183.0, 64.0, 0.0, 0.0, 23.3, 0.672, 32.0] => 1 (expected 1)\n",
      "[1.0, 89.0, 66.0, 23.0, 94.0, 28.1, 0.167, 21.0] => 0 (expected 0)\n",
      "[0.0, 137.0, 40.0, 35.0, 168.0, 43.1, 2.288, 33.0] => 0 (expected 1)\n",
      "[5.0, 116.0, 74.0, 0.0, 0.0, 25.6, 0.201, 30.0] => 0 (expected 0)\n",
      "[3.0, 78.0, 50.0, 32.0, 88.0, 31.0, 0.248, 26.0] => 0 (expected 1)\n",
      "[10.0, 115.0, 0.0, 0.0, 0.0, 35.3, 0.134, 29.0] => 1 (expected 0)\n",
      "[2.0, 197.0, 70.0, 45.0, 543.0, 30.5, 0.158, 53.0] => 0 (expected 1)\n",
      "[8.0, 125.0, 96.0, 0.0, 0.0, 0.0, 0.232, 54.0] => 0 (expected 1)\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print('%s => %d (expected %d)' % (X[i].tolist(), predictions[i], y[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ajuste el modelo\n",
    "Cambie la configuración del modelo o el proceso de entrenamiento y vea si puede mejorar el rendimiento del modelo, p. Ej. lograr una precisión superior al 76%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
